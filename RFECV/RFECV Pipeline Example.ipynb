{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import clone\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data (just for demonstration purposes)\n",
    "X, y = make_classification(n_samples=1000, n_features=30, n_informative=10, random_state=42)\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple variant\n",
    "**Do a hyperparameter optimization and then refit an RFECV + XGB with the best hyperparameters again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train):\n",
    "\n",
    "    # XGB hyperparameters (for better list of hyperparameters see https://github.com/optuna/optuna-examples/tree/main/xgboost)\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10)\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier(**params)\n",
    "\n",
    "    # Combine the classifier with the scaler. This has the advantage that the scaler will be fit\n",
    "    # automatically with the correct training data in every fold of the cross-validation in the RFECV\n",
    "    classifier_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"classifier\", xgb_model)\n",
    "    ])\n",
    "\n",
    "    # If you're doing this with a pipeline, you need to give RFECV the importance_getter\n",
    "    importance_getter = \"named_steps.classifier.feature_importances_\"\n",
    "\n",
    "    classifier = RFECV(classifier_pipeline, cv=3, n_jobs=4, scoring=\"f1\", importance_getter=importance_getter)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # The RFECV already performs a cross-validation, so no custom validation set is needed!\n",
    "    # This way the model used in the RFECV is exactly the same (same hyperparams) as the model used for evaluation.\n",
    "    # We can just get the mean score for the best number of features (which is stored in n_features_ after fitting).\n",
    "    # This line only works if rfecv.step==1 (I think, havent tested it with other values)\n",
    "    score = classifier.cv_results_[\"mean_test_score\"][classifier.n_features_ - 1] \n",
    "    \n",
    "    trial.set_user_attr(\"n_features\", classifier.n_features_) # Just for logging\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-12 17:34:39,491] A new study created in memory with name: no-name-230f6837-1331-4589-9c76-54e0db442183\n",
      "[I 2024-10-12 17:34:47,110] Trial 3 finished with value: 0.8460200957085044 and parameters: {'learning_rate': 0.05243446457557887, 'n_estimators': 279, 'max_depth': 1}. Best is trial 3 with value: 0.8460200957085044.\n",
      "[I 2024-10-12 17:34:53,980] Trial 1 finished with value: 0.8421207139703824 and parameters: {'learning_rate': 0.0015335301046056164, 'n_estimators': 303, 'max_depth': 8}. Best is trial 3 with value: 0.8460200957085044.\n",
      "[I 2024-10-12 17:34:54,862] Trial 0 finished with value: 0.9002548463326043 and parameters: {'learning_rate': 0.028566888124578346, 'n_estimators': 602, 'max_depth': 2}. Best is trial 0 with value: 0.9002548463326043.\n",
      "[I 2024-10-12 17:34:56,766] Trial 2 finished with value: 0.8160865853895296 and parameters: {'learning_rate': 0.0072765397472806855, 'n_estimators': 850, 'max_depth': 1}. Best is trial 0 with value: 0.9002548463326043.\n",
      "[I 2024-10-12 17:35:25,734] Trial 4 finished with value: 0.8991040569964209 and parameters: {'learning_rate': 0.004221647602339382, 'n_estimators': 838, 'max_depth': 10}. Best is trial 0 with value: 0.9002548463326043.\n",
      "[I 2024-10-12 17:35:26,684] Trial 5 finished with value: 0.8618941272085289 and parameters: {'learning_rate': 0.0015475252596133776, 'n_estimators': 460, 'max_depth': 10}. Best is trial 0 with value: 0.9002548463326043.\n",
      "[I 2024-10-12 17:35:31,001] Trial 7 finished with value: 0.9183201058201057 and parameters: {'learning_rate': 0.08786799369768078, 'n_estimators': 250, 'max_depth': 7}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:35:38,698] Trial 8 finished with value: 0.8524628469025058 and parameters: {'learning_rate': 0.0025936741925127508, 'n_estimators': 260, 'max_depth': 6}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:35:49,425] Trial 9 finished with value: 0.9079329228397551 and parameters: {'learning_rate': 0.03645435857668433, 'n_estimators': 783, 'max_depth': 2}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:35:52,283] Trial 6 finished with value: 0.8329063438438439 and parameters: {'learning_rate': 0.0005464617120548963, 'n_estimators': 540, 'max_depth': 7}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:06,200] Trial 10 finished with value: 0.9050082533147282 and parameters: {'learning_rate': 0.010287394511210595, 'n_estimators': 777, 'max_depth': 5}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:15,171] Trial 13 finished with value: 0.7961124184507069 and parameters: {'learning_rate': 0.00020359419810431103, 'n_estimators': 110, 'max_depth': 4}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:17,668] Trial 14 finished with value: 0.9091549376952427 and parameters: {'learning_rate': 0.08242671875570239, 'n_estimators': 125, 'max_depth': 4}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:30,104] Trial 11 finished with value: 0.8220196285045324 and parameters: {'learning_rate': 0.00012323275928100835, 'n_estimators': 519, 'max_depth': 8}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:31,850] Trial 16 finished with value: 0.9075284861347827 and parameters: {'learning_rate': 0.07698621271922843, 'n_estimators': 142, 'max_depth': 4}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:34,119] Trial 12 finished with value: 0.908986309266465 and parameters: {'learning_rate': 0.015818433143735926, 'n_estimators': 681, 'max_depth': 9}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:34,136] Trial 15 finished with value: 0.9172982931047446 and parameters: {'learning_rate': 0.09067123719155354, 'n_estimators': 703, 'max_depth': 3}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:34,979] Trial 17 finished with value: 0.9104776769673015 and parameters: {'learning_rate': 0.08531474141232406, 'n_estimators': 101, 'max_depth': 4}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:42,009] Trial 18 finished with value: 0.9051381563908301 and parameters: {'learning_rate': 0.018483841776540014, 'n_estimators': 329, 'max_depth': 4}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:36:54,855] Trial 19 finished with value: 0.9106467793418943 and parameters: {'learning_rate': 0.09018513110070908, 'n_estimators': 976, 'max_depth': 5}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:37:12,825] Trial 20 finished with value: 0.9153739113005651 and parameters: {'learning_rate': 0.020796255071382514, 'n_estimators': 988, 'max_depth': 6}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:37:28,098] Trial 21 finished with value: 0.9103479093520588 and parameters: {'learning_rate': 0.01908075083728372, 'n_estimators': 978, 'max_depth': 6}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:37:33,012] Trial 23 finished with value: 0.9130676367040004 and parameters: {'learning_rate': 0.03512094811819483, 'n_estimators': 443, 'max_depth': 6}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:37:38,283] Trial 22 finished with value: 0.9129442892151044 and parameters: {'learning_rate': 0.03093426791284599, 'n_estimators': 957, 'max_depth': 6}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:37:54,573] Trial 24 finished with value: 0.9116370167478235 and parameters: {'learning_rate': 0.03274132316696145, 'n_estimators': 953, 'max_depth': 6}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:37:55,787] Trial 25 finished with value: 0.9129120119161613 and parameters: {'learning_rate': 0.04066535491541198, 'n_estimators': 414, 'max_depth': 7}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:38:05,026] Trial 26 finished with value: 0.9102030602030601 and parameters: {'learning_rate': 0.043319099658914915, 'n_estimators': 655, 'max_depth': 7}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:38:13,438] Trial 27 finished with value: 0.9158515373654815 and parameters: {'learning_rate': 0.04762431051307033, 'n_estimators': 700, 'max_depth': 7}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:38:27,436] Trial 29 finished with value: 0.8949313039642024 and parameters: {'learning_rate': 0.008343270415672248, 'n_estimators': 657, 'max_depth': 3}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:38:36,127] Trial 28 finished with value: 0.9040840202717787 and parameters: {'learning_rate': 0.007516399543754721, 'n_estimators': 657, 'max_depth': 7}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:38:37,329] Trial 30 finished with value: 0.8963050481609708 and parameters: {'learning_rate': 0.009551366181919792, 'n_estimators': 687, 'max_depth': 3}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:38:55,551] Trial 32 finished with value: 0.910084084084084 and parameters: {'learning_rate': 0.06021261443711843, 'n_estimators': 614, 'max_depth': 8}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:39:00,933] Trial 31 finished with value: 0.906794583227201 and parameters: {'learning_rate': 0.009952797845801024, 'n_estimators': 688, 'max_depth': 8}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:39:11,578] Trial 33 finished with value: 0.9153672418713912 and parameters: {'learning_rate': 0.06580082472300178, 'n_estimators': 749, 'max_depth': 8}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:39:15,395] Trial 34 finished with value: 0.9103607819230707 and parameters: {'learning_rate': 0.05332519181984998, 'n_estimators': 596, 'max_depth': 8}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:39:29,097] Trial 35 finished with value: 0.9120663191809437 and parameters: {'learning_rate': 0.022772668986490795, 'n_estimators': 749, 'max_depth': 9}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:39:43,460] Trial 36 finished with value: 0.9117590685872742 and parameters: {'learning_rate': 0.017990169869837366, 'n_estimators': 874, 'max_depth': 5}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:39:57,844] Trial 37 finished with value: 0.9116608072638915 and parameters: {'learning_rate': 0.022375208934651617, 'n_estimators': 861, 'max_depth': 5}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:02,302] Trial 38 finished with value: 0.9114164944903581 and parameters: {'learning_rate': 0.017900226146941, 'n_estimators': 882, 'max_depth': 5}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:09,357] Trial 41 finished with value: 0.78935368956743 and parameters: {'learning_rate': 0.003124589267706421, 'n_estimators': 197, 'max_depth': 2}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:14,222] Trial 42 finished with value: 0.7384911175130878 and parameters: {'learning_rate': 0.004617203643150623, 'n_estimators': 355, 'max_depth': 1}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:17,707] Trial 40 finished with value: 0.7964405261469047 and parameters: {'learning_rate': 0.00416827080854237, 'n_estimators': 901, 'max_depth': 1}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:17,944] Trial 43 finished with value: 0.8695040319373568 and parameters: {'learning_rate': 0.09734403236258223, 'n_estimators': 505, 'max_depth': 1}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:22,853] Trial 39 finished with value: 0.8996257982120052 and parameters: {'learning_rate': 0.005143117358970197, 'n_estimators': 861, 'max_depth': 5}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:34,480] Trial 44 finished with value: 0.9115100262668974 and parameters: {'learning_rate': 0.06085374583440966, 'n_estimators': 734, 'max_depth': 9}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:45,697] Trial 45 finished with value: 0.9109089630684316 and parameters: {'learning_rate': 0.0601441879389452, 'n_estimators': 739, 'max_depth': 9}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:56,759] Trial 46 finished with value: 0.9132193732193733 and parameters: {'learning_rate': 0.05663942611637714, 'n_estimators': 741, 'max_depth': 9}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:40:58,355] Trial 47 finished with value: 0.9126949566076777 and parameters: {'learning_rate': 0.05375745574709423, 'n_estimators': 740, 'max_depth': 9}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:41:07,521] Trial 48 finished with value: 0.9129714912847442 and parameters: {'learning_rate': 0.06356719992165791, 'n_estimators': 810, 'max_depth': 7}. Best is trial 7 with value: 0.9183201058201057.\n",
      "[I 2024-10-12 17:41:18,573] Trial 49 finished with value: 0.8661421744901355 and parameters: {'learning_rate': 0.000911719831421655, 'n_estimators': 815, 'max_depth': 7}. Best is trial 7 with value: 0.9183201058201057.\n"
     ]
    }
   ],
   "source": [
    "# Do the hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train), n_jobs=4, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.924\n",
      "Number of features: 17\n"
     ]
    }
   ],
   "source": [
    "# Refit the model and RFECV with the best hyperparameters on all training data\n",
    "best_params = study.best_params\n",
    "xgb_model = XGBClassifier(**best_params)\n",
    "classifier_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"classifier\", xgb_model)\n",
    "])\n",
    "\n",
    "# The RFECV itself is an sklearn estimator! It can be used a normal classifier.\n",
    "classifier = RFECV(classifier_pipeline, cv=3, n_jobs=4, scoring=\"f1\", importance_getter=\"named_steps.classifier.feature_importances_\")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# Again: The RFECV object can be used similarly to a normal model\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Number of features: {classifier.n_features_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fancy variant\n",
    "I've found a way to optimize the refit part: The exact RFECV refit in the cell above has already been done in one of the optuna trials.  \n",
    "**So we can save the best classifier during the optimization and just access it later.**  \n",
    "You can do this using a global variable or by saving the object to disk as a file. I first thought that a global variable doesn't work because of multithreading and implemented it with files. But global variables should actually work too and are simpler.  \n",
    "As mentioned, optuna usually runs multithreaded, i.e. with multiple trials running at the same time.  \n",
    "So you have to synchronize access to the file/variable with a [`threading.Lock`](https://docs.python.org/3/library/threading.html#lock-objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = None\n",
    "best_score = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train, lock):\n",
    "\n",
    "    global best_classifier\n",
    "    global best_score\n",
    "\n",
    "    # XGB hyperparameters (for better list of hyperparameters see https://github.com/optuna/optuna-examples/tree/main/xgboost)\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10)\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier(**params)\n",
    "\n",
    "    # Combine the classifier with the scaler. This has the advantage that the scaler will be fit\n",
    "    # automatically with the correct training data in every fold of the cross-validation in the RFECV\n",
    "    classifier_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"classifier\", xgb_model)\n",
    "    ])\n",
    "    # If you're doing this with a pipeline, you need to give RFECV the importance_getter\n",
    "    importance_getter = \"named_steps.classifier.feature_importances_\"\n",
    "\n",
    "    classifier = RFECV(classifier_pipeline, cv=3, n_jobs=1, scoring=\"f1\", importance_getter=importance_getter)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # The RFECV already performs a cross-validation, so we can just get the mean score \n",
    "    # for the best number of features (which is the one stored in n_features_ after fitting)\n",
    "    # This line only works if rfecv.step==1 (I think, havent tested it with other values)\n",
    "    score = classifier.cv_results_[\"mean_test_score\"][classifier.n_features_ - 1] \n",
    "\n",
    "    with lock: # Exclusive to avoid race conditions and lost updates with the other optuna threads\n",
    "        if score > best_score:\n",
    "            # Save best classifier\n",
    "            best_classifier = classifier\n",
    "            best_score = score\n",
    "    \n",
    "    trial.set_user_attr(\"n_features\", classifier.n_features_)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-12 18:33:13,694] A new study created in memory with name: no-name-139224cb-e533-432f-bfef-b19c649d4fad\n",
      "[I 2024-10-12 18:34:22,460] Trial 6 finished with value: 0.6898220962091953 and parameters: {'learning_rate': 0.0005989028123519016, 'n_estimators': 462, 'max_depth': 1}. Best is trial 6 with value: 0.6898220962091953.\n",
      "[I 2024-10-12 18:34:31,289] Trial 10 finished with value: 0.7973090503003516 and parameters: {'learning_rate': 0.0001025082173794712, 'n_estimators': 226, 'max_depth': 3}. Best is trial 10 with value: 0.7973090503003516.\n",
      "[I 2024-10-12 18:34:48,163] Trial 12 finished with value: 0.9105669703617049 and parameters: {'learning_rate': 0.05527663090631521, 'n_estimators': 148, 'max_depth': 10}. Best is trial 12 with value: 0.9105669703617049.\n",
      "[I 2024-10-12 18:34:51,882] Trial 9 finished with value: 0.8977680421174047 and parameters: {'learning_rate': 0.019275236694670378, 'n_estimators': 339, 'max_depth': 3}. Best is trial 12 with value: 0.9105669703617049.\n",
      "[I 2024-10-12 18:34:54,976] Trial 0 finished with value: 0.8998792914821311 and parameters: {'learning_rate': 0.031329374530886724, 'n_estimators': 491, 'max_depth': 2}. Best is trial 12 with value: 0.9105669703617049.\n",
      "[I 2024-10-12 18:35:06,429] Trial 11 finished with value: 0.7971108494876488 and parameters: {'learning_rate': 0.00016183212605394397, 'n_estimators': 249, 'max_depth': 4}. Best is trial 12 with value: 0.9105669703617049.\n",
      "[I 2024-10-12 18:35:27,757] Trial 5 finished with value: 0.68887027566128 and parameters: {'learning_rate': 0.00019458157727691164, 'n_estimators': 922, 'max_depth': 1}. Best is trial 12 with value: 0.9105669703617049.\n",
      "[I 2024-10-12 18:36:15,168] Trial 17 finished with value: 0.8612838158761894 and parameters: {'learning_rate': 0.027241597177787843, 'n_estimators': 849, 'max_depth': 1}. Best is trial 12 with value: 0.9105669703617049.\n",
      "[I 2024-10-12 18:36:25,571] Trial 19 finished with value: 0.7833528043215608 and parameters: {'learning_rate': 0.0002528820539633792, 'n_estimators': 414, 'max_depth': 2}. Best is trial 12 with value: 0.9105669703617049.\n",
      "[I 2024-10-12 18:36:30,453] Trial 13 finished with value: 0.9021724498488698 and parameters: {'learning_rate': 0.013566790722790477, 'n_estimators': 658, 'max_depth': 3}. Best is trial 12 with value: 0.9105669703617049.\n",
      "[I 2024-10-12 18:36:33,860] Trial 21 finished with value: 0.8815417765855532 and parameters: {'learning_rate': 0.014677721069548554, 'n_estimators': 430, 'max_depth': 2}. Best is trial 12 with value: 0.9105669703617049.\n",
      "[I 2024-10-12 18:36:42,041] Trial 14 finished with value: 0.9128881335655897 and parameters: {'learning_rate': 0.08011259603710903, 'n_estimators': 487, 'max_depth': 9}. Best is trial 14 with value: 0.9128881335655897.\n",
      "[I 2024-10-12 18:36:44,938] Trial 15 finished with value: 0.8939614744626683 and parameters: {'learning_rate': 0.007456428113394842, 'n_estimators': 810, 'max_depth': 3}. Best is trial 14 with value: 0.9128881335655897.\n",
      "[I 2024-10-12 18:37:13,438] Trial 8 finished with value: 0.8876447909898202 and parameters: {'learning_rate': 0.0076407681687011425, 'n_estimators': 303, 'max_depth': 10}. Best is trial 14 with value: 0.9128881335655897.\n",
      "[I 2024-10-12 18:37:32,961] Trial 3 finished with value: 0.8186169268229794 and parameters: {'learning_rate': 0.00012903146272682392, 'n_estimators': 274, 'max_depth': 9}. Best is trial 14 with value: 0.9128881335655897.\n",
      "[I 2024-10-12 18:37:38,873] Trial 23 finished with value: 0.8940528968067695 and parameters: {'learning_rate': 0.028787631676384356, 'n_estimators': 100, 'max_depth': 9}. Best is trial 14 with value: 0.9128881335655897.\n",
      "[I 2024-10-12 18:37:42,280] Trial 28 finished with value: 0.9106220280792762 and parameters: {'learning_rate': 0.0967135112349213, 'n_estimators': 107, 'max_depth': 10}. Best is trial 14 with value: 0.9128881335655897.\n",
      "[I 2024-10-12 18:37:44,247] Trial 27 finished with value: 0.9138590203106332 and parameters: {'learning_rate': 0.09271798851274095, 'n_estimators': 109, 'max_depth': 10}. Best is trial 27 with value: 0.9138590203106332.\n",
      "[I 2024-10-12 18:37:49,785] Trial 25 finished with value: 0.9116571605472025 and parameters: {'learning_rate': 0.08563595688725556, 'n_estimators': 131, 'max_depth': 10}. Best is trial 27 with value: 0.9138590203106332.\n",
      "[I 2024-10-12 18:37:59,142] Trial 16 finished with value: 0.9054283020786807 and parameters: {'learning_rate': 0.016347738212623114, 'n_estimators': 746, 'max_depth': 3}. Best is trial 27 with value: 0.9138590203106332.\n",
      "[I 2024-10-12 18:38:28,538] Trial 20 finished with value: 0.7833536298372605 and parameters: {'learning_rate': 0.00029055805642636597, 'n_estimators': 966, 'max_depth': 2}. Best is trial 27 with value: 0.9138590203106332.\n",
      "[I 2024-10-12 18:38:32,632] Trial 29 finished with value: 0.9124264349930843 and parameters: {'learning_rate': 0.08806927254936794, 'n_estimators': 129, 'max_depth': 10}. Best is trial 27 with value: 0.9138590203106332.\n",
      "[I 2024-10-12 18:38:43,016] Trial 30 finished with value: 0.9106945165305822 and parameters: {'learning_rate': 0.08284806300729172, 'n_estimators': 122, 'max_depth': 7}. Best is trial 27 with value: 0.9138590203106332.\n",
      "[I 2024-10-12 18:40:01,755] Trial 26 finished with value: 0.910157639977554 and parameters: {'learning_rate': 0.07866599341364815, 'n_estimators': 690, 'max_depth': 10}. Best is trial 27 with value: 0.9138590203106332.\n",
      "[I 2024-10-12 18:40:16,550] Trial 22 finished with value: 0.9108419543202152 and parameters: {'learning_rate': 0.0758830974553919, 'n_estimators': 882, 'max_depth': 6}. Best is trial 27 with value: 0.9138590203106332.\n",
      "[I 2024-10-12 18:40:52,122] Trial 31 finished with value: 0.91632923501253 and parameters: {'learning_rate': 0.09770171955326384, 'n_estimators': 593, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:41:15,969] Trial 32 finished with value: 0.9135728234088889 and parameters: {'learning_rate': 0.09737349382213076, 'n_estimators': 666, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:41:40,673] Trial 24 finished with value: 0.9134369148685542 and parameters: {'learning_rate': 0.06073349324740177, 'n_estimators': 854, 'max_depth': 10}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:41:50,788] Trial 36 finished with value: 0.912970173488267 and parameters: {'learning_rate': 0.09534335120314624, 'n_estimators': 613, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:42:22,474] Trial 7 finished with value: 0.8484737484737485 and parameters: {'learning_rate': 0.0007097391089776487, 'n_estimators': 750, 'max_depth': 9}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:42:32,468] Trial 1 finished with value: 0.8466742614765493 and parameters: {'learning_rate': 0.0007431404061751902, 'n_estimators': 672, 'max_depth': 10}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:42:35,422] Trial 18 finished with value: 0.8309619468746039 and parameters: {'learning_rate': 0.0005448155982960595, 'n_estimators': 617, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:44:54,997] Trial 41 finished with value: 0.9127798775781267 and parameters: {'learning_rate': 0.04145324370029065, 'n_estimators': 567, 'max_depth': 8}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:45:06,023] Trial 4 finished with value: 0.8726201749345432 and parameters: {'learning_rate': 0.0015692058420743262, 'n_estimators': 823, 'max_depth': 9}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:45:24,643] Trial 34 finished with value: 0.8713218601841549 and parameters: {'learning_rate': 0.0016924470348599144, 'n_estimators': 641, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:45:37,560] Trial 38 finished with value: 0.870548069523596 and parameters: {'learning_rate': 0.0017813256330841464, 'n_estimators': 646, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:46:07,205] Trial 33 finished with value: 0.8713218601841549 and parameters: {'learning_rate': 0.001560191650452267, 'n_estimators': 697, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:46:15,094] Trial 35 finished with value: 0.8690649813644465 and parameters: {'learning_rate': 0.0014250149551920468, 'n_estimators': 689, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:46:16,530] Trial 39 finished with value: 0.8712231361730152 and parameters: {'learning_rate': 0.002120189539722637, 'n_estimators': 557, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:46:19,154] Trial 37 finished with value: 0.8690649813644465 and parameters: {'learning_rate': 0.001440466966670894, 'n_estimators': 654, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:46:33,730] Trial 47 finished with value: 0.9133255993885724 and parameters: {'learning_rate': 0.0450509720270335, 'n_estimators': 544, 'max_depth': 8}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:46:42,538] Trial 46 finished with value: 0.912970173488267 and parameters: {'learning_rate': 0.040313087464496966, 'n_estimators': 576, 'max_depth': 8}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:47:22,005] Trial 2 finished with value: 0.8264662867082491 and parameters: {'learning_rate': 0.00015922541749954013, 'n_estimators': 956, 'max_depth': 10}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:47:22,783] Trial 49 finished with value: 0.9124561025272487 and parameters: {'learning_rate': 0.04922372788993724, 'n_estimators': 554, 'max_depth': 5}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:47:25,276] Trial 44 finished with value: 0.8716085012381308 and parameters: {'learning_rate': 0.0022590376604043364, 'n_estimators': 560, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:47:29,606] Trial 40 finished with value: 0.8751688825782024 and parameters: {'learning_rate': 0.0022788342567873106, 'n_estimators': 601, 'max_depth': 8}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:47:38,551] Trial 48 finished with value: 0.9102071928006015 and parameters: {'learning_rate': 0.045197309338494154, 'n_estimators': 526, 'max_depth': 5}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:47:40,716] Trial 42 finished with value: 0.8742540038836335 and parameters: {'learning_rate': 0.001964940347064381, 'n_estimators': 651, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:47:43,065] Trial 45 finished with value: 0.8755615364077945 and parameters: {'learning_rate': 0.002396278166412522, 'n_estimators': 556, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n",
      "[I 2024-10-12 18:47:43,284] Trial 43 finished with value: 0.8414641926983256 and parameters: {'learning_rate': 0.0007655170640515568, 'n_estimators': 588, 'max_depth': 7}. Best is trial 31 with value: 0.91632923501253.\n"
     ]
    }
   ],
   "source": [
    "# Do the hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "lock = threading.Lock()\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train, lock), n_jobs=16, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92       123\n",
      "           1       0.91      0.94      0.92       127\n",
      "\n",
      "    accuracy                           0.92       250\n",
      "   macro avg       0.92      0.92      0.92       250\n",
      "weighted avg       0.92      0.92      0.92       250\n",
      "\n",
      "Number of features: 18\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Number of features: {best_classifier.n_features_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train, temp_results_path, lock):\n",
    "\n",
    "    # XGB hyperparameters (for better list of hyperparameters see https://github.com/optuna/optuna-examples/tree/main/xgboost)\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10)\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier(**params)\n",
    "\n",
    "    # Combine the classifier with the scaler. This has the advantage that the scaler will be fit\n",
    "    # automatically with the correct training data in every fold of the cross-validation in the RFECV\n",
    "    classifier_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"classifier\", xgb_model)\n",
    "    ])\n",
    "    # If you're doing this with a pipeline, you need to give RFECV the importance_getter\n",
    "    importance_getter = \"named_steps.classifier.feature_importances_\"\n",
    "\n",
    "    classifier = RFECV(classifier_pipeline, cv=3, n_jobs=4, scoring=\"f1\", importance_getter=importance_getter)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # The RFECV already performs a cross-validation, so we can just get the mean score \n",
    "    # for the best number of features (which is the one stored in n_features_ after fitting)\n",
    "    # This line only works if rfecv.step==1 (I think, havent tested it with other values)\n",
    "    score = classifier.cv_results_[\"mean_test_score\"][classifier.n_features_ - 1] \n",
    "\n",
    "    with lock: # Exclusive to avoid race conditions and lost updates with the other optuna threads\n",
    "        # Get current best score\n",
    "        files = os.listdir(temp_results_path)\n",
    "        if len(files) == 1:\n",
    "            # Convert file name to score\n",
    "            best_score = float(files[0].split(\"_\")[-1].split(\".\")[0])\n",
    "        elif len(files) == 0:\n",
    "            best_score = -1\n",
    "        else:\n",
    "            raise ValueError(f\"More than one file in {temp_results_path}! {files}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            if best_score != -1:\n",
    "                # Delete old best estimator\n",
    "                os.remove(os.path.join(temp_results_path, files[0]))\n",
    "            # Save best estimator\n",
    "            path = os.path.join(temp_results_path, f\"temp_best_estimator_{score}.pkl\")\n",
    "            with open(path, 'wb') as f:\n",
    "                # Saves the thing that we would retrain ourselves in the simpler variant\n",
    "                pickle.dump(classifier, f)\n",
    "    \n",
    "    trial.set_user_attr(\"n_features\", classifier.n_features_)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-12 18:02:27,307] A new study created in memory with name: no-name-13e8378d-b9f6-4922-9b6b-4b372e72b383\n",
      "[I 2024-10-12 18:02:42,536] Trial 0 finished with value: 0.9076228333955191 and parameters: {'learning_rate': 0.04539687366328101, 'n_estimators': 594, 'max_depth': 2}. Best is trial 0 with value: 0.9076228333955191.\n",
      "[I 2024-10-12 18:02:49,846] Trial 1 finished with value: 0.7837392249185079 and parameters: {'learning_rate': 0.0007979452630045476, 'n_estimators': 482, 'max_depth': 2}. Best is trial 0 with value: 0.9076228333955191.\n",
      "[I 2024-10-12 18:02:53,753] Trial 2 finished with value: 0.8959800183365397 and parameters: {'learning_rate': 0.005892803155073984, 'n_estimators': 815, 'max_depth': 3}. Best is trial 0 with value: 0.9076228333955191.\n",
      "[I 2024-10-12 18:03:00,768] Trial 3 finished with value: 0.9137847222222222 and parameters: {'learning_rate': 0.01954442022129439, 'n_estimators': 930, 'max_depth': 5}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:03:25,080] Trial 4 finished with value: 0.8828175783176967 and parameters: {'learning_rate': 0.0018006576755656397, 'n_estimators': 912, 'max_depth': 9}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:03:30,245] Trial 5 finished with value: 0.9091898146588476 and parameters: {'learning_rate': 0.023457482752436015, 'n_estimators': 543, 'max_depth': 10}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:03:33,013] Trial 6 finished with value: 0.9133219373219373 and parameters: {'learning_rate': 0.052867039922025516, 'n_estimators': 706, 'max_depth': 6}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:03:46,821] Trial 8 finished with value: 0.9129796627143638 and parameters: {'learning_rate': 0.0701337142286508, 'n_estimators': 348, 'max_depth': 6}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:03:52,556] Trial 7 finished with value: 0.8277162598437076 and parameters: {'learning_rate': 0.00032554738957752446, 'n_estimators': 529, 'max_depth': 6}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:03:53,821] Trial 10 finished with value: 0.8890417646345403 and parameters: {'learning_rate': 0.01931103026154492, 'n_estimators': 101, 'max_depth': 6}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:04:08,675] Trial 12 finished with value: 0.8025833815307499 and parameters: {'learning_rate': 0.00011947038860857756, 'n_estimators': 103, 'max_depth': 6}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:04:12,385] Trial 9 finished with value: 0.8996814540388209 and parameters: {'learning_rate': 0.007432847353301643, 'n_estimators': 567, 'max_depth': 10}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:04:32,682] Trial 11 finished with value: 0.8072718982792493 and parameters: {'learning_rate': 0.0001533761488579776, 'n_estimators': 777, 'max_depth': 5}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:04:48,496] Trial 13 finished with value: 0.8999766225152667 and parameters: {'learning_rate': 0.005691836228840176, 'n_estimators': 983, 'max_depth': 4}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:04:54,493] Trial 14 finished with value: 0.9036851991895244 and parameters: {'learning_rate': 0.01020305907971826, 'n_estimators': 747, 'max_depth': 4}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:05:04,525] Trial 15 finished with value: 0.9130577462984483 and parameters: {'learning_rate': 0.08999969381900369, 'n_estimators': 997, 'max_depth': 4}. Best is trial 3 with value: 0.9137847222222222.\n",
      "[I 2024-10-12 18:05:10,155] Trial 16 finished with value: 0.9143671691240401 and parameters: {'learning_rate': 0.09081046463112337, 'n_estimators': 1000, 'max_depth': 4}. Best is trial 16 with value: 0.9143671691240401.\n",
      "[I 2024-10-12 18:05:24,773] Trial 17 finished with value: 0.9118458571555276 and parameters: {'learning_rate': 0.01873329469924564, 'n_estimators': 729, 'max_depth': 8}. Best is trial 16 with value: 0.9143671691240401.\n",
      "[I 2024-10-12 18:05:28,303] Trial 18 finished with value: 0.9113436613851551 and parameters: {'learning_rate': 0.07730295883591835, 'n_estimators': 691, 'max_depth': 8}. Best is trial 16 with value: 0.9143671691240401.\n",
      "[I 2024-10-12 18:05:42,515] Trial 19 finished with value: 0.9106337742701379 and parameters: {'learning_rate': 0.023868171091698913, 'n_estimators': 689, 'max_depth': 8}. Best is trial 16 with value: 0.9143671691240401.\n",
      "[I 2024-10-12 18:05:50,615] Trial 21 finished with value: 0.7583410531561153 and parameters: {'learning_rate': 0.0023862310433347064, 'n_estimators': 898, 'max_depth': 1}. Best is trial 16 with value: 0.9143671691240401.\n",
      "[I 2024-10-12 18:05:57,159] Trial 20 finished with value: 0.9157839665677736 and parameters: {'learning_rate': 0.02230231837361021, 'n_estimators': 881, 'max_depth': 8}. Best is trial 20 with value: 0.9157839665677736.\n",
      "[I 2024-10-12 18:06:04,794] Trial 22 finished with value: 0.8666088582312561 and parameters: {'learning_rate': 0.002425506770407596, 'n_estimators': 882, 'max_depth': 3}. Best is trial 20 with value: 0.9157839665677736.\n",
      "[I 2024-10-12 18:06:08,530] Trial 23 finished with value: 0.762011695781868 and parameters: {'learning_rate': 0.00267160255472367, 'n_estimators': 876, 'max_depth': 1}. Best is trial 20 with value: 0.9157839665677736.\n",
      "[I 2024-10-12 18:06:22,171] Trial 24 finished with value: 0.9175782437236463 and parameters: {'learning_rate': 0.04123853752341211, 'n_estimators': 865, 'max_depth': 5}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:06:37,556] Trial 25 finished with value: 0.9149120704174273 and parameters: {'learning_rate': 0.035501258030115924, 'n_estimators': 882, 'max_depth': 7}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:06:50,156] Trial 26 finished with value: 0.9143668496298463 and parameters: {'learning_rate': 0.038580278461505836, 'n_estimators': 854, 'max_depth': 7}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:06:58,796] Trial 27 finished with value: 0.9128773407938602 and parameters: {'learning_rate': 0.04257468132268891, 'n_estimators': 958, 'max_depth': 7}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:07:07,855] Trial 28 finished with value: 0.9121528936343751 and parameters: {'learning_rate': 0.038503357453678014, 'n_estimators': 822, 'max_depth': 7}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:07:20,587] Trial 29 finished with value: 0.9136198319552215 and parameters: {'learning_rate': 0.03751841313407836, 'n_estimators': 826, 'max_depth': 7}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:07:37,698] Trial 30 finished with value: 0.9085237513808943 and parameters: {'learning_rate': 0.011037910004162673, 'n_estimators': 631, 'max_depth': 7}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:07:48,026] Trial 32 finished with value: 0.9013227378080507 and parameters: {'learning_rate': 0.011229230051313878, 'n_estimators': 350, 'max_depth': 9}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:07:51,513] Trial 31 finished with value: 0.9052629180024004 and parameters: {'learning_rate': 0.00981786038621081, 'n_estimators': 625, 'max_depth': 7}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:07:58,930] Trial 33 finished with value: 0.9109953881064787 and parameters: {'learning_rate': 0.013129109419550956, 'n_estimators': 619, 'max_depth': 9}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:08:09,864] Trial 34 finished with value: 0.9147639885993923 and parameters: {'learning_rate': 0.09798227244247969, 'n_estimators': 934, 'max_depth': 9}. Best is trial 24 with value: 0.9175782437236463.\n",
      "[I 2024-10-12 18:08:22,996] Trial 36 finished with value: 0.9177908050015615 and parameters: {'learning_rate': 0.09220342325824273, 'n_estimators': 952, 'max_depth': 3}. Best is trial 36 with value: 0.9177908050015615.\n",
      "[I 2024-10-12 18:08:23,642] Trial 35 finished with value: 0.9130982543428803 and parameters: {'learning_rate': 0.058943893424816235, 'n_estimators': 943, 'max_depth': 5}. Best is trial 36 with value: 0.9177908050015615.\n",
      "[I 2024-10-12 18:08:35,489] Trial 37 finished with value: 0.9156991101517437 and parameters: {'learning_rate': 0.06210433228346204, 'n_estimators': 948, 'max_depth': 5}. Best is trial 36 with value: 0.9177908050015615.\n",
      "[I 2024-10-12 18:08:46,305] Trial 38 finished with value: 0.9091376871503721 and parameters: {'learning_rate': 0.060233612861027006, 'n_estimators': 789, 'max_depth': 8}. Best is trial 36 with value: 0.9177908050015615.\n",
      "[I 2024-10-12 18:08:51,434] Trial 39 finished with value: 0.8051856945854392 and parameters: {'learning_rate': 0.0012402436419811933, 'n_estimators': 796, 'max_depth': 2}. Best is trial 36 with value: 0.9177908050015615.\n",
      "[I 2024-10-12 18:08:55,235] Trial 40 finished with value: 0.8030717779677058 and parameters: {'learning_rate': 0.0011685027860060095, 'n_estimators': 783, 'max_depth': 2}. Best is trial 36 with value: 0.9177908050015615.\n",
      "[I 2024-10-12 18:09:06,231] Trial 41 finished with value: 0.9195845276029742 and parameters: {'learning_rate': 0.02675150546938058, 'n_estimators': 780, 'max_depth': 3}. Best is trial 41 with value: 0.9195845276029742.\n",
      "[I 2024-10-12 18:09:07,153] Trial 42 finished with value: 0.8635490795771261 and parameters: {'learning_rate': 0.004141434634147582, 'n_estimators': 475, 'max_depth': 3}. Best is trial 41 with value: 0.9195845276029742.\n",
      "[I 2024-10-12 18:09:12,392] Trial 43 finished with value: 0.8566068961084868 and parameters: {'learning_rate': 0.004342480844074222, 'n_estimators': 424, 'max_depth': 3}. Best is trial 41 with value: 0.9195845276029742.\n",
      "[I 2024-10-12 18:09:23,598] Trial 44 finished with value: 0.915303221381477 and parameters: {'learning_rate': 0.03054340693246655, 'n_estimators': 912, 'max_depth': 3}. Best is trial 41 with value: 0.9195845276029742.\n",
      "[I 2024-10-12 18:09:34,000] Trial 45 finished with value: 0.9209924292297563 and parameters: {'learning_rate': 0.026741880290180268, 'n_estimators': 921, 'max_depth': 3}. Best is trial 45 with value: 0.9209924292297563.\n",
      "[I 2024-10-12 18:09:43,967] Trial 46 finished with value: 0.9145955513064569 and parameters: {'learning_rate': 0.02565097919491292, 'n_estimators': 922, 'max_depth': 3}. Best is trial 45 with value: 0.9209924292297563.\n",
      "[I 2024-10-12 18:09:57,542] Trial 47 finished with value: 0.9166945315909457 and parameters: {'learning_rate': 0.02445798630092839, 'n_estimators': 918, 'max_depth': 5}. Best is trial 45 with value: 0.9209924292297563.\n",
      "[I 2024-10-12 18:10:06,146] Trial 49 finished with value: 0.901297932864065 and parameters: {'learning_rate': 0.025705626470453855, 'n_estimators': 846, 'max_depth': 2}. Best is trial 45 with value: 0.9209924292297563.\n",
      "[I 2024-10-12 18:10:07,820] Trial 48 finished with value: 0.9153872267578212 and parameters: {'learning_rate': 0.023936596453910636, 'n_estimators': 949, 'max_depth': 5}. Best is trial 45 with value: 0.9209924292297563.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "temp_results_path = \"temp_results\" # Your path here\n",
    "os.makedirs(temp_results_path, exist_ok=True)\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Do the hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train, temp_results_path, lock), n_jobs=4, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.94\n",
      "Number of features: 14\n"
     ]
    }
   ],
   "source": [
    "# Now just load the best classifier, no new RFECV necessary\n",
    "path = os.path.join(temp_results_path, os.listdir(temp_results_path)[0])\n",
    "with open(path, 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Number of features: {classifier.n_features_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HiWi_Benedikt_Wille_Neuer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
