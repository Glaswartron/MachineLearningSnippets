{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, optimization and test of XGBoost for regression with different approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content:\n",
    "\n",
    "1. Importing and preparing data\n",
    "2. Training a simple model without hyperparameter optimization and cross-validation\n",
    "3. Basic hyperparameter optimization with cross-validation\n",
    "4. Hyperparameter optimization with Optuna and cross-validation\n",
    "5. Hyperparameter optimization with Optuna and cross-validation with stratification by bins\n",
    "6. Nested cross-validation with Optuna for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"path_to_your_pickled_dataframe\"\n",
    "\n",
    "features = ['Your feature column names go here']\n",
    "\n",
    "target = \"Your target column name goes here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path)\n",
    "\n",
    "X = df[features]\n",
    "y = df[target].dropna()\n",
    "X = X.loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a simple model without hyperparameter optimization and cross-validation\n",
    "\n",
    "Workflow: Split data into train and test sets, scale data, train model, evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Fit only on training data\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit model\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Test model on test set\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(\"RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic hyperparameter optimization with cross-validation\n",
    "\n",
    "With train_test_split and RandomizedSearchCV.  \n",
    "\n",
    "Optuna is better/more state-of-the-art than RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Fit only on training data\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# The model to use\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# Generated by GitHub Copilot, maybe double-check ;)\n",
    "params = {\n",
    "    \"n_estimators\": [100, 500, 1000, 2000],\n",
    "    \"max_depth\": [3, 5, 10, 20],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"colsample_bytree\": [0.3, 0.5, 0.7, 0.9, 1],\n",
    "    \"subsample\": [0.3, 0.5, 0.7, 0.9, 1],\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"min_child_weight\": [0.5, 1, 3, 5, 7],\n",
    "    \"reg_alpha\": [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"reg_lambda\": [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "# Optimize XGBoost model using RandomizedSearchCV (using 100 iterations and 5-fold cross-validation)\n",
    "xgb_cv = RandomizedSearchCV(xgb, params, n_iter=100, cv=5, scoring=\"neg_mean_squared_error\", random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train model with best hyperparameters on all training data\n",
    "xgb_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test model on test set\n",
    "y_pred = xgb_cv.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization with Optuna and cross-validation\n",
    "\n",
    "With train_test_split, Optuna and cross_val_score.  \n",
    "\n",
    "The is the \"standard run\" to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Fit only on training data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # See https://github.com/optuna/optuna-examples/\n",
    "\n",
    "    params = {\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster\n",
    "        # \"gblinear\" also exists but doesn't work on the dataset\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]), \n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0)\n",
    "    }\n",
    "\n",
    "    if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "\n",
    "        # defines how selective algorithm is.\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if params[\"booster\"] == \"dart\":\n",
    "        params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    # ---\n",
    "\n",
    "    # Setup model with hyperparameters for this trial\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # Cross-validation: Try different training/validation splits (5-fold CV)\n",
    "    neg_rmses = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    neg_mean_rmse = neg_rmses.mean()\n",
    "\n",
    "    return neg_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "study = optuna.create_study(direction=\"maximize\") # Maximize negative RMSE = minimize RMSE\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1) # n_trials=100 -> 100 iterations, n_jobs=-1 -> use all CPU cores\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train model with best hyperparameters on all training data\n",
    "xgb = XGBRegressor(**best_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Test model on test set\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization with Optuna and cross-validation with stratification by bins\n",
    "\n",
    "With train_test_split, Optuna and StratifiedKFold.  \n",
    "\n",
    "Variation of the approach above. Only relevant for regression problems.\n",
    "\n",
    "The y values are being split into bins (like in a histogram).  \n",
    "Then for every split it is ensured that the distribution of the bins in the splits is as equal as possible/is preserved (stratification).  \n",
    "This is important if the distribution of the values is not uniform.  \n",
    "In the objective function you have to do the splits yourself with StratifiedKFold, because cross_val_score afaik does not support stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.cut(y, bins=10, labels=np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=bins, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "# Preserve structure of dataframes (needed later for stratified k-fold)\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index) # Fit only on training data\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # See https://github.com/optuna/optuna-examples/\n",
    "\n",
    "    params = {\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster\n",
    "        # \"gblinear\" also exists but doesn't work on the dataset\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]), \n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0)\n",
    "    }\n",
    "\n",
    "    if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "\n",
    "        # defines how selective algorithm is.\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if params[\"booster\"] == \"dart\":\n",
    "        params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    # ---\n",
    "\n",
    "    # Setup model with hyperparameters for this trial\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # Cross-validation: Try different training/validation splits (5-fold CV)\n",
    "    neg_rmses = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_index, valid_index in skf.split(X_train, bins.loc[X_train.index]):\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        y_pred = model.predict(X_valid_fold)\n",
    "        neg_rmses.append(-1 * mean_squared_error(y_valid_fold, y_pred, squared=False))\n",
    "\n",
    "    neg_mean_rmse = np.mean(neg_rmses)\n",
    "\n",
    "    return neg_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train model with best hyperparameters on all training data\n",
    "xgb = XGBRegressor(**best_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Test model on test set\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested cross-validation with Optuna for hyperparameter optimization\n",
    "\n",
    "With KFold, cross_val_score and Optuna.  \n",
    "\n",
    "Nested cross-validation involves varying both the train-validation and train-test splits.  \n",
    "This means there is an additional loop for different train-test splits.  \n",
    "Nested cross-validation provides more robust metrics and allows us to assess the standard deviation of model performance.  \n",
    "However, it also takes longer and is more computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train):\n",
    "\n",
    "    # See https://github.com/optuna/optuna-examples/\n",
    "\n",
    "    params = {\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster\n",
    "        # \"gblinear\" also exists but doesn't work on the dataset\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]), \n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0)\n",
    "    }\n",
    "\n",
    "    if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "\n",
    "        # defines how selective algorithm is.\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if params[\"booster\"] == \"dart\":\n",
    "        params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    # ---\n",
    "\n",
    "    # Setup model with hyperparameters for this trial\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # Inner cross-validation: Try different training/validation splits (5-fold CV)\n",
    "    neg_rmses = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    neg_mean_rmse = neg_rmses.mean()\n",
    "\n",
    "    return neg_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = KFold(5)\n",
    "\n",
    "rmses = []\n",
    "\n",
    "# Outer cross-validation loop: Try different train/test splits (5-fold CV)\n",
    "for train_index, test_index in outer_cv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train) # Fit only on training data\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Optimize hyperparameters on the train set using Optuna\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    objective_fold = lambda trial: objective(trial, X_train, y_train) # Specialized objective function with X_train and y_train fixed\n",
    "    study.optimize(objective_fold, n_trials=100, n_jobs=-1)\n",
    "\n",
    "    # Train model on the full train set using the best hyperparameters found above\n",
    "    best_params = study.best_params\n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "    # Record results for this fold/split\n",
    "    rmses.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {np.mean(rmses)} +/- {np.std(rmses)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Benedikt_Wille_Hiwi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
