{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Optimierung und Test von XGBoost zur Regression mit verschiedenen Ansätzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inhalt:\n",
    "\n",
    "1. Daten importieren und vorbereiten\n",
    "2. Einfaches Modell trainieren ohne Hyperparameter-Optimierung und Cross-Validation\n",
    "3. Normale Hyperparameter-Optimierung mit Cross-Validation\n",
    "4. Hyperparamter-Optimierung mit Optuna und Cross-Validation\n",
    "5. Hyperparamter-Optimierung mit Optuna und Cross-Validation mit Stratifizierung nach Bins\n",
    "6. Nested Cross-Validation mit Optuna zur Hyperparameter-Optimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten importieren und vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Pfad zu deinem DataFrame als pickle File\"\n",
    "\n",
    "features = [\"Namen der Feature Spalten\"]\n",
    "\n",
    "target = \"Name der Zielspalte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path)\n",
    "\n",
    "X = df[features]\n",
    "y = df[target].dropna()\n",
    "X = X.loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfaches Modell trainieren ohne Hyperparameter-Optimierung und Cross-Validation\n",
    "\n",
    "Workflow: Daten in Train- und Test-Set splitten, Daten skalieren, Modell trainieren, Modell evaluieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Fit only on training data\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit model\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Test model on test set\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(\"RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normale Hyperparameter-Optimierung mit Cross-Validation\n",
    "\n",
    "Mit train_test_split und RandomizedSearchCV.  \n",
    "\n",
    "Optuna ist besser/mehr state-of-the-art als RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Fit only on training data\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train XGBoost using RandomizedSearchCV\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# Generated by GitHub Copilot, maybe double-check ;)\n",
    "params = {\n",
    "    \"n_estimators\": [100, 500, 1000, 2000],\n",
    "    \"max_depth\": [3, 5, 10, 20],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"colsample_bytree\": [0.3, 0.5, 0.7, 0.9, 1],\n",
    "    \"subsample\": [0.3, 0.5, 0.7, 0.9, 1],\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"min_child_weight\": [0.5, 1, 3, 5, 7],\n",
    "    \"reg_alpha\": [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"reg_lambda\": [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning (5-fold CV)\n",
    "xgb_cv = RandomizedSearchCV(xgb, params, n_iter=100, cv=5, scoring=\"neg_mean_squared_error\", random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train model with best hyperparameters on all training data\n",
    "xgb_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test model on test set\n",
    "y_pred = xgb_cv.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter-Optimierung mit Optuna und Cross-Validation\n",
    "\n",
    "Mit train_test_split, Optuna und cross_val_score.  \n",
    "\n",
    "Das hier ist der \"Standard-Run\", den man verwenden sollte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Fit only on training data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # See https://github.com/optuna/optuna-examples/\n",
    "\n",
    "    params = {\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster\n",
    "        # \"gblinear\" also exists but doesn't work on the dataset\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]), \n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0)\n",
    "    }\n",
    "\n",
    "    if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "\n",
    "        # defines how selective algorithm is.\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if params[\"booster\"] == \"dart\":\n",
    "        params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    # ---\n",
    "\n",
    "    # Setup model with hyperparameters for this trial\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # Cross-validation: Try different training/validation splits (5-fold CV)\n",
    "    neg_rmses = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    neg_mean_rmse = neg_rmses.mean()\n",
    "\n",
    "    return neg_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "study = optuna.create_study(direction=\"maximize\") # Maximize negative RMSE = minimize RMSE\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1) # n_trials=100 -> 100 iterations, n_jobs=-1 -> use all CPU cores\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train model with best hyperparameters on all training data\n",
    "xgb = XGBRegressor(**best_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Test model on test set\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter-Optimierung mit Optuna und Cross-Validation mit Stratifizierung nach Bins\n",
    "\n",
    "Mit train_test_split, Optuna und StratifiedKFold.  \n",
    "\n",
    "Abwandlung von dem Ansatz drüber.\n",
    "\n",
    "Die y-Werte werden in Bins eingeteilt (wie bei einem Histogramm).  \n",
    "Dann wird bei jedem Split darauf geachtet, dass die Verteilung der Bins in den Splits möglichst gleich ist/erhalten bleibt (Stratifizierung).  \n",
    "Das ist wichtig, wenn die Verteilung der Werte nicht gleichmäßig ist.  \n",
    "In der objective-Funktion muss man hier die Splits selber machen mit StratifiedKFold, weil cross_val_score afaik keine Stratifizierung unterstützt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.cut(y, bins=10, labels=np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=bins, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "# Preserve structure of dataframes (needed later for stratified k-fold)\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index) # Fit only on training data\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # See https://github.com/optuna/optuna-examples/\n",
    "\n",
    "    params = {\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster\n",
    "        # \"gblinear\" also exists but doesn't work on the dataset\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]), \n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0)\n",
    "    }\n",
    "\n",
    "    if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "\n",
    "        # defines how selective algorithm is.\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if params[\"booster\"] == \"dart\":\n",
    "        params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    # ---\n",
    "\n",
    "    # Setup model with hyperparameters for this trial\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # Cross-validation: Try different training/validation splits (5-fold CV)\n",
    "    neg_rmses = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_index, valid_index in skf.split(X_train, bins.loc[X_train.index]):\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        y_pred = model.predict(X_valid_fold)\n",
    "        neg_rmses.append(-1 * mean_squared_error(y_valid_fold, y_pred, squared=False))\n",
    "\n",
    "    neg_mean_rmse = np.mean(neg_rmses)\n",
    "\n",
    "    return neg_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train model with best hyperparameters on all training data\n",
    "xgb = XGBRegressor(**best_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Test model on test set\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation mit Optuna zur Hyperparameter-Optimierung\n",
    "\n",
    "Mit KFold, cross_val_score und Optuna.  \n",
    "\n",
    "Nested Cross-Validation heißt, dass sowohl die Train-Validation- als auch die Train-Test-Splits variiert werden.  \n",
    "Es gibt also außenrum nochmal eine Schleife für verschiedene Train-Test-Splits.  \n",
    "Durch die Nested Cross-Validation werden die Ergebnisse robuster und man erfährt die Standardabweichung der Modell-Performance,  \n",
    "es dauert aber auch deutlich länger und ist aufwändiger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train):\n",
    "\n",
    "    # See https://github.com/optuna/optuna-examples/\n",
    "\n",
    "    params = {\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster\n",
    "        # \"gblinear\" also exists but doesn't work on the dataset\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]), \n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0)\n",
    "    }\n",
    "\n",
    "    if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "\n",
    "        # defines how selective algorithm is.\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if params[\"booster\"] == \"dart\":\n",
    "        params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    # ---\n",
    "\n",
    "    # Setup model with hyperparameters for this trial\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # Inner cross-validation: Try different training/validation splits (5-fold CV)\n",
    "    neg_rmses = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    neg_mean_rmse = neg_rmses.mean()\n",
    "\n",
    "    return neg_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = KFold(5)\n",
    "\n",
    "rmses = []\n",
    "\n",
    "# Outer cross-validation loop: Try different train/test splits (5-fold CV)\n",
    "for train_index, test_index in outer_cv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train) # Fit only on training data\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Optimize hyperparameters on the train set using Optuna\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    objective_fold = lambda trial: objective(trial, X_train, y_train) # Specialized objective function with X_train and y_train fixed\n",
    "    study.optimize(objective_fold, n_trials=100, n_jobs=-1)\n",
    "\n",
    "    # Train model on the full train set using the best hyperparameters found above\n",
    "    best_params = study.best_params\n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "    # Record results for this fold/split\n",
    "    rmses.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {np.mean(rmses)} +/- {np.std(rmses)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Benedikt_Wille_Hiwi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
